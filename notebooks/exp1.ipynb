{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e985fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import mlflow.sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa4911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52d4aaa",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "1. Lemmatize\n",
    "2. Stop words removal\n",
    "3. Removing numbers\n",
    "4. Removing punctuations\n",
    "5. Converting to lowercase\n",
    "6. Removing URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba20b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Lemmatization function\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join(lemmatizer.lemmatize(word) for word in text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8df720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Stop words removal function\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join(word for word in text.split() if word.lower() not in stop_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bdb11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Remove numbers\n",
    "def remove_numbers(text):\n",
    "    return ''.join([char for char in text if not char.isdigit()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996a35bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Convert to lowercase\n",
    "def to_lowercase(text):\n",
    "    return \" \".join(word.lower() for word in text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618539a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Remove punctuations\n",
    "def remove_punctuation(text):\n",
    "    text = re.sub('[%s]' %re.escape(string.punctuation), ' ', text)\n",
    "    text = text.replace(';', ' ')\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71245a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Remove URLs\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c77893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(df):\n",
    "    try:\n",
    "        df['review'] = df['review'].apply(to_lowercase)\n",
    "        df['review'] = df['review'].apply(remove_stopwords)\n",
    "        df['review'] = df['review'].apply(remove_numbers)\n",
    "        df['review'] = df['review'].apply(remove_punctuation)\n",
    "        df['review'] = df['review'].apply(remove_urls)\n",
    "        df['review'] = df['review'].apply(lemmatize_text)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f'Error during text normalization: {e}')\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3606487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = normalize_text(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9eb51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd468e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['sentiment'].isin(['positive', 'negative'])\n",
    "df = df[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aa9d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the sentiment labels to binary values: positive -> 1, negative -> 0\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative':0})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acaace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62158e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=100)\n",
    "X = vectorizer.fit_transform(df['review'])\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6523b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95adcad4",
   "metadata": {},
   "source": [
    "## Add MLFlow via Dagshub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a902af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dagshub\n",
    "\n",
    "mlflow.set_tracking_uri('<link to your dagshub project/repo>')\n",
    "dagshub.init(repo_owner = '<dagshub repo owner>', repo_name = '<dagshub repo name>', mlflow = True)\n",
    "\n",
    "mlflow.set_experiment('Logistic Regression Experiment (Baseline)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca651fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa646a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level = logging.INFO, format= \"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logging.info('Starting MLFlow experiment...')\n",
    "with mlflow.start_run():\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        logging.info('Logging preprocessing parameters...')\n",
    "        mlflow.log_param('vectorizer', 'Bag of words')\n",
    "        mlflow.log_param('num_features', 100)\n",
    "        mlflow.log_param('test_size', 0.25)\n",
    "\n",
    "        logging.info('Initializing logistic regression model...')\n",
    "        model = LogisticRegression(max_iter=1000) # Increased max_iter to prevent non-convergence\n",
    "\n",
    "        logging.info('Training the model...')\n",
    "        model.fit(X_train, y_train)\n",
    "        logging.info('Model training completed.')\n",
    "\n",
    "        logging.info('Logging model parameters...')\n",
    "        mlflow.log_param('model', 'LogisticRegression')\n",
    "\n",
    "        logging.info('Making predictions on the test set...')\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        logging.info('Calculating evaluation metrics...')\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        logging.info('Logging evaluation metrics...')\n",
    "        mlflow.log_metric('accuracy', accuracy)\n",
    "        mlflow.log_metric('precision', precision)\n",
    "        mlflow.log_metric('recall', recall)\n",
    "        mlflow.log_metric('f1_score', f1)\n",
    "\n",
    "        logging.info('Saving and logging the model...')\n",
    "        #mlflow.sklearn.log_model(model, 'Logistic_Regression_Model')\n",
    "\n",
    "        # Log the duration of the run\n",
    "        logging.info(f'Model training, logging and evaluation completed in {time.time() - start_time:.2f} seconds')\n",
    "\n",
    "        # Print out the metrics\n",
    "        logging.info(f'Accuracy: {accuracy:.4f}')\n",
    "        logging.info(f'Precision: {precision:.4f}')\n",
    "        logging.info(f'Recall: {recall:.4f}')\n",
    "        logging.info(f'F1 Score: {f1:.4f}')\n",
    "    except Exception as e:\n",
    "        logging.error(f'An error occured: {e}', exc_info=True)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
